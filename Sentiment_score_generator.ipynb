{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "miniature-shakespeare",
   "metadata": {},
   "source": [
    "# Calculation of sentiment score for Disney fandom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-lambda",
   "metadata": {},
   "source": [
    "The code in this notebook is used for calculating the sentiment scores of the Disney fandom pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alike-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import urllib.request \n",
    "import json \n",
    "import re \n",
    "import math\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "import nltk, pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "multiple-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment functions\n",
    "\n",
    "#set a tokenizing pattern\n",
    "def my_tokenize(text):\n",
    "    pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.             # ellipsis\n",
    "      | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "    '''\n",
    "    return nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "def tokenize_and_lower(raw):\n",
    "    tokens=my_tokenize(raw)\n",
    "    cleaned_tokens=[token for token in tokens if re.search('\\w*',token)]\n",
    "    minimized_tokens=[w.lower() for w in cleaned_tokens]\n",
    "    minimized_tokens  \n",
    "    return minimized_tokens\n",
    "\n",
    "def tokenize_pages(all_pages):\n",
    "    tokened_pages=[]\n",
    "    maximum=len(all_pages)\n",
    "    i=0\n",
    "    for char,text in all_pages:\n",
    "        print(f\"\\rcalculating {((i*(1/maximum)*100)):5.2f}% completed, please wait\", end = ' ')\n",
    "        tokened_pages.append([char,tokenize_and_lower(text)])\n",
    "        i+=1\n",
    "    print(f\"\\rcalculating {100:5.2f}% completed, Done!               \")\n",
    "    return tokened_pages\n",
    "\n",
    "def import_Sentiment_file():\n",
    "    text_file = open(\"Data_Set_S1\"+\".txt\", \"r\", encoding= 'utf-8')\n",
    "    lines = text_file.readlines()\n",
    "    lines= [line[:-2] for line in lines[4:]]\n",
    "    wordset=[]\n",
    "    for line in lines:\n",
    "        word=line.split(\"\\t\")[0]\n",
    "        score=float(line.split(\"\\t\")[2])\n",
    "        wordset.append([word,score])\n",
    "    return wordset\n",
    "\n",
    "\n",
    "def calculate_sentiment(tokenlist):\n",
    "    tokenlist= [token.lower() for token in tokenlist] #make lowercase\n",
    "    lookup_list= import_Sentiment_file()     #get datasetlist\n",
    "    sentiment=0\n",
    "    words=0\n",
    "    distributed_text=nltk.FreqDist(tokenlist)\n",
    "    for token in distributed_text:\n",
    "        for (lookupname,value) in lookup_list:\n",
    "            if (token==lookupname):\n",
    "                sentiment+=distributed_text[token]*value\n",
    "                words+=distributed_text[token]\n",
    "    return sentiment/words\n",
    "    \n",
    "def calculate_all_sentiment(universe):\n",
    "    sentiment=[]\n",
    "    percent=0.0;\n",
    "    i=0\n",
    "    maximum=len(universe)\n",
    "    for hero,text in universe:\n",
    "        print(f\"\\rcalculating {((i*(1/maximum)*100)):5.2f}% completed, please wait\", end = ' ')\n",
    "        sentiment.append([hero,calculate_sentiment(text)])\n",
    "        i+=1\n",
    "    print(f\"\\rcalculating {100:5.2f}% completed, Done!               \")\n",
    "    save_to_sentiment_file(sentiment,f\"sentiment_{universe[0][0]}\")\n",
    "    return sentiment\n",
    "# save to sentiment file\n",
    "\n",
    "def save_to_sentiment_file(sentiment_in,filename,path=\"\"):\n",
    "    string=\"\"\n",
    "    for name,score in sentiment_in:\n",
    "        string+=name+\"|'|\"+str(score)+\"|||\"\n",
    "    write_to_file(filename,string,path)\n",
    "    print(f\"sentiment scores saved as {filename}\")    \n",
    "    \n",
    "\n",
    "\n",
    "#open sentiment from file\n",
    "def open_sentiment_from_file(filename):\n",
    "    sentiment_file=open_file(filename)\n",
    "    string=sentiment_file[0]\n",
    "    sentiments= string.split(\"|||\")\n",
    "    sen_from_file=[[name.split(\"|'|\")[0],float(name.split(\"|'|\")[1])] for name in sentiments[:-1]]\n",
    "    return sen_from_file\n",
    "\n",
    "def universes_sentiment(universes,sentiment):\n",
    "    universe_sentiment_scores=[]\n",
    "    for name,universe in universes:\n",
    "        universe_sentiment_scores.append([name, [[name,score] for name,score in sentiment if name in universe]] )\n",
    "    return universe_sentiment_scores\n",
    "\n",
    "def group_character_with_pages(character_list,path=\"CharacterPages//\"):\n",
    "    all_pages=[]\n",
    "    for name in character_list:\n",
    "        filename = path + name + \".txt\"\n",
    "        try:\n",
    "            string = open(filename, \"r\", encoding=\"utf-8\").read()\n",
    "            words = string\n",
    "            all_pages.append([name.replace(\"SEMICOLON\",\":\"),words])\n",
    "        except:\n",
    "            continue\n",
    "            #print(f\"Error with {name}\")\n",
    "    return all_pages\n",
    "\n",
    "def open_file(filename,place=\"\"):\n",
    "    text_file = open(place+filename.replace(\":\",\"SEMICOLON\")+\".txt\", \"r\", encoding= 'utf-8') # ':' can't be part of filename\n",
    "    return text_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535af014",
   "metadata": {},
   "source": [
    "## Generate list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f541774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All characters\n",
    "t = os.listdir(\"completelist/\")\n",
    "Characters_pages = [w[0:-4] for w in t]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508abee5",
   "metadata": {},
   "source": [
    "## Group all characters with their files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f4b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pages=group_character_with_pages(Characters_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b274c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating 100.00% completed, Done!                                                                           \n"
     ]
    }
   ],
   "source": [
    "all_pages_tokenized=tokenize_pages(all_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425bb4c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Warning! takes about 1 hour, Use the next function if this have already run\n",
    "sentiment=calculate_all_sentiment(all_pages_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08434d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get sentiment from a file\n",
    "sentiment_from_file=open_sentiment_from_file(\"sentiment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
