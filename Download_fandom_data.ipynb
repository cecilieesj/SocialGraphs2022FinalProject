{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "resistant-latin",
   "metadata": {},
   "source": [
    "# Downloading of Disney fandom data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-single",
   "metadata": {},
   "source": [
    "The code in this notebook is used for downloading all the Disney-fandom wiki pages and making lists of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "powerful-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import urllib.request \n",
    "import json \n",
    "import re \n",
    "import math\n",
    "import os\n",
    "from urllib.parse import quote\n",
    "import nltk, pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "piano-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using api from disney.fandom\n",
    "# input: title of page to get\n",
    "#output: json object of the page\n",
    "def getpage(title,fandom=\"disney\"):\n",
    "    baseurl = f\"https://{fandom}.fandom.com/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = f\"titles={quote(title)}\"\n",
    "    content = \"prop=revisions&rvprop=content\"\n",
    "    dataformat =\"format=json\"\n",
    "    rvslots= \"rvslots=main\"\n",
    "    query = \"{}{}&{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat,rvslots)\n",
    "    #print(query)\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "    wikidata = (wikiresponse.read())\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "    jsontext =json.loads(wikitext)\n",
    "    return jsontext\n",
    "\n",
    "def getpagetext(title,fandom=\"disney\"):\n",
    "    #print(title)\n",
    "    jsontext=getpage(title,fandom)\n",
    "    key=jsontext[\"query\"][\"pages\"].keys()\n",
    "    webpagetext=\"0\"\n",
    "    for ints in key:\n",
    "        key1=jsontext[\"query\"][\"pages\"][ints]\n",
    "        if ('revisions' in key1): \n",
    "            webpagetext=key1['revisions'][0]['slots']['main']['*']\n",
    "        else:\n",
    "            print(f\"error with {title} \")\n",
    "    return webpagetext\n",
    "#Writes a string to a file\n",
    "#input: Filename, string, optional path.\n",
    "#output: None\n",
    "#example:  write_to_file(\"testfile\",\"testtext\")\n",
    "def write_to_file(filename,text,place=\"\"):\n",
    "    if (len(text) >1000):\n",
    "        #open text file\n",
    "        text_file = open(place+filename.replace(\":\",\"SEMICOLON\")+\".txt\", \"w\", encoding= 'utf-8') # ':' can't be part of filename\n",
    "        #print(\"saving in: \"+place+filename+\".txt\")\n",
    "        #write string to file\n",
    "        text_file.write(text)\n",
    "        #close file\n",
    "        text_file.close()\n",
    "def open_file(filename,place=\"\"):\n",
    "    text_file = open(place+filename.replace(\":\",\"SEMICOLON\")+\".txt\", \"r\", encoding= 'utf-8') # ':' can't be part of filename\n",
    "    return text_file.readlines()\n",
    "    \n",
    "#Downloads a webpage from the fandom and saves it locally\n",
    "#input: Page to download\n",
    "#output:none\n",
    "#example create_character_file(\"Goofy\")\n",
    "def create_character_file(character):\n",
    "    text=getpagetext(character)\n",
    "    write_to_file(character,text)\n",
    "\n",
    "#Using API to get all pages from a category, excluding the new categories found.\n",
    "#input: A category\n",
    "#output: a list of all the names found in the category, excluding any categories.\n",
    "#example \n",
    "def get_all_names_from_category(category_in,fandom=\"disney\"):\n",
    "    names=[]\n",
    "    continues=\"\"\n",
    "    baseurl = f\"https://{fandom}.fandom.com/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    content = \"prop=revisions&rvprop=content\"\n",
    "    dataformat =\"format=json\"\n",
    "    formatversion=\"formatversion=2\"\n",
    "    lists=\"list=categorymembers\"\n",
    "    cmlimit=\"cmlimit=500\"\n",
    "    category=f\"cmtitle=Category%3A{category_in}\"\n",
    "    while (True):\n",
    "        cmcontinue=f\"cmcontinue={continues}\"\n",
    "        query = \"{}{}&{}&{}&{}&{}&{}&{}\".format(baseurl,action,dataformat,formatversion,lists,cmlimit,category,cmcontinue)\n",
    "    \n",
    "        wikiresponse = urllib.request.urlopen(query)\n",
    "        wikidata = (wikiresponse.read())\n",
    "        wikitext = wikidata.decode('utf-8')\n",
    "        jsontext =json.loads(wikitext)\n",
    "        for i in range(len(jsontext['query']['categorymembers'])): #append found names to names[]\n",
    "            names.append(jsontext['query']['categorymembers'][i]['title'])\n",
    "        try: #look for cmcontinue for next page\n",
    "            continues=  jsontext['continue']['cmcontinue']\n",
    "        except:\n",
    "            break\n",
    "    cleaned=[name for name in names if name[0:8]!='Category']\n",
    "    print(f\"found {len(cleaned)} in category:{category_in}\")\n",
    "    return cleaned\n",
    "# Get all names from a category and write them to a file\n",
    "# input: category to get names from\n",
    "# output: list of names from category\n",
    "# example: save_characters_from_category_to_file(\"Villains\",category)\n",
    "def save_characters_from_category_to_file(category,path=\"\",fandom=\"disney\"):\n",
    "    returned=get_all_names_from_category(category,fandom)\n",
    "    write_to_file(category,\"|||\".join(returned),path)\n",
    "    return returned\n",
    "\n",
    "# download all pages from a list of pagenames using API\n",
    "# input: list of pages,optional path to save files\n",
    "# output: list of pages that gave errors\n",
    "# example: get_pages_from_list(['22', 'A-Li', 'Abby Park', 'Princess Abigail', 'Ada'])\n",
    "def get_pages_from_list(character_list,path=\"\",fandom=\"disney\"):\n",
    "    search=\"\"\n",
    "    error_titles=[]\n",
    "    length=len(character_list)\n",
    "    print(length)\n",
    "    for i in range (length):\n",
    "        print(f\"\\rcalculating {((i*(1/length)*100)):5.2f}% completed, please wait\", end = ' ')\n",
    "        search+=character_list[i]+'|'\n",
    "        if (i%49==48 or i==(length-1)):\n",
    "            result=getpage(search,fandom)\n",
    "            keys=result['query']['pages'].keys()\n",
    "            for key in keys:\n",
    "                if int(key)>0:\n",
    "                    content=result['query']['pages'][key]['revisions'][0]['slots']['main']['*']\n",
    "                    title=result['query']['pages'][key]['title']\n",
    "                    try:\n",
    "                        write_to_file(title.replace(\"/\",\"\"),content,path)\n",
    "                    except:\n",
    "                        error_titles.append(title)\n",
    "                    search=\"\"\n",
    "            \n",
    "    print(f\"\\rcalculating {100:5.2f}% completed, done!               \")\n",
    "    return error_titles\n",
    "\n",
    "#Exstracts the films from a characters textfile\n",
    "# input: filename\n",
    "# output: list of films character is present in\n",
    "# example: get_films_from_character_file(\"Goofy.txt\")\n",
    "def get_films_from_character_file(filename):\n",
    "    string = open(filename, \"r\", encoding=\"utf-8\").read()\n",
    "    string=string.split('\\n')\n",
    "    for line in string:\n",
    "        if line[:5]=='|film':\n",
    "            filter= r'\\[\\[([\\w]*[\\s\\w*]*)\\]\\]'\n",
    "            match = re.findall(filter, line)\n",
    "            return match\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-clearance",
   "metadata": {},
   "source": [
    "## Generate a list of all characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d5a2b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 11483 in category:Characters\n"
     ]
    }
   ],
   "source": [
    "Category=\"Characters\"\n",
    "Characters=save_characters_from_category_to_file(Category,\"Universes//\"+Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1758f",
   "metadata": {},
   "source": [
    "## Generate gender lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8057cd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 9726 in category:Males\n",
      "found 5341 in category:Females\n"
     ]
    }
   ],
   "source": [
    "Category=\"Males\"\n",
    "Males=save_characters_from_category_to_file(Category,\"Universes//\"+Category)\n",
    "Category=\"Females\"\n",
    "Females=save_characters_from_category_to_file(Category,\"Universes//\"+Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f2d0e",
   "metadata": {},
   "source": [
    "## Generating universe lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352aa5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Category=\"Star_Wars_characters\"\n",
    "Star_Wars_characters=save_characters_from_category_to_file(Category,Category)\n",
    "Category=\"Marvel_Comics_characters\"\n",
    "Marvel_Comics_characters=save_characters_from_category_to_file(Category,\"Universes//\"+Category)\n",
    "Category=\"Pixar_characters\"\n",
    "Pixar_characters=save_characters_from_category_to_file(Category,\"Universes//\"+Category)\n",
    "Category=\"Disney_characters\"\n",
    "Disney_characters=save_characters_from_category_to_file(Category,\"Universes//\"+Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05100ec",
   "metadata": {},
   "source": [
    "## Generate Villians and heroes/heroines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "Category=\"Villains\"\n",
    "Villains=save_characters_from_category_to_file(Category,\"Allignment//\"+Category)\n",
    "Villains_female=[Villain for Villain in Villains if Villain in Females]\n",
    "Villains_male=[Villain for Villain in Villains if Villain in Males]\n",
    "Villains_undefined=[Villain for Villain in Villains if Villain not in (Villains_female+Villains_male)]\n",
    "\n",
    "Category=\"Heroes\"\n",
    "Heroes=save_characters_from_category_to_file(Category,\"Allignment//\"+Category)\n",
    "Category=\"Heroines\"\n",
    "Heroines=save_characters_from_category_to_file(Category,\"Allignment//\"+Category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5d9fd",
   "metadata": {},
   "source": [
    "## Downloading all character pages and saving locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Error_pages=get_pages_from_list(Characters,\"CharacterPages//\")\n",
    "print(f\"Done downloading and saving, {len(Error_pages)} downloads had an error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
